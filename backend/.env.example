# LLM Configuration
LLM_PROVIDER=ollama  # Options: ollama, deepseek
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=deepseek-r1:1.5b  # or qwen2.5:7b, llama3.1:8b, etc.

# DeepSeek API Configuration (if using cloud)
# DEEPSEEK_API_KEY=your_api_key_here
# DEEPSEEK_BASE_URL=https://api.deepseek.com

# MCP Sandbox Configuration
SANDBOX_CONTAINER_NAME=sandbox-sandbox-os-1
MCP_PYTHON_PATH=/opt/mcp-venv/bin/python
MCP_SERVERS_DIR=/opt/mcp-servers

# Backend API Configuration
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000
CORS_ORIGINS=http://localhost:5173,http://localhost:3000

# Logging
LOG_LEVEL=INFO
